---
title: "WhatsApp analysis"
output: html_notebook
---

#load packages
```{r}
library(ggthemes)
library(janitor)
library(knitr) #for knitting tables
library(tidyverse)
library(wordcloud)
library(qdap) # text mining: freq_terms
library(tm) # text mining: converts vectors to Source objects; preprocessing/cleaning functions
library(tidytext) # text mining: tokenizes easily
devtools::install_github("JBGruber/rwhatsapp")
library("rwhatsapp")
library(SnowballC) ##provides a wordStem() function, which for a character
# vector, returns the same vector of terms in its root form. For example, the function
# correctly stems the variants of the word learn, as described previously
```


#load the whatsapp text file and create the Corpus object
```{r}
texts <- readLines("whatsapp.txt")
texts2 <- readLines("whatsapp.txt")

#create the corpus using tm
docs <- Corpus(VectorSource(texts))

docs[[2]]$content # double brackets to get metadata on second tweet; $content (or single brackets)
```

#Apply common preporocessing functions
```{r}
##applying preprocessing steps to a corpus using tm package
### using tm_map() makes it easier to apply cleaning functions to an entire corpus

# create a custom function so that you can apply the same functions over multiple corpora
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower)) # use content_transformer when using base R functions like tolower()
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "hannah guedenet", "media","omit", "charles","omitted"))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  #corpus <- tm_map(corpus, stemDocument)
  return(corpus)
}

#apply preprocessing function to textCorpus (corpus of qual data)
clean_corp <- clean_corpus(docs)

## see how it's cleaned up one of the stories
clean_corp[[250]]$content

# content_transformer function modify the content of an R object.
# content_transformer is used to create a wrapper to get and set the content of text documents
# These steps replace common signs with a space
trans <- content_transformer(function(x, pattern) gsub(pattern, "", x))
clean_corp <- tm_map(clean_corp, trans, "/")
clean_corp <- tm_map(clean_corp, trans, "@")
clean_corp <- tm_map(clean_corp, trans, "\\|")

clean_corp[[22]]$content
```


## create TermDocumentMatrix (TDM) or DocumentTermMatrix (DTM) to get frequency matrices of corpus
```{r}
qual_tdm <- TermDocumentMatrix(clean_corp)
dim(qual_tdm)
inspect(qual_tdm[1:5,1:6])

#use RemoveSparseTerms()

## convert dtm to matrix so that it's easier to look at and manipulate
qualtdm_matrix <- as.matrix(qual_tdm)
qualtdm_matrix[1:4,1:10]
```

#calculate total frequency of terms and sort in decreasing order
```{r}
freqTerms <- rowSums(qualtdm_matrix) %>% sort(., decreasing = TRUE)
head(freqTerms)

# turn into a dataframe
freqTermsdf <- freqTerms %>% data.frame()
names(freqTermsdf) <- "number"
freqTermsdf <- freqTermsdf %>% mutate(terms = row.names(.)) %>% arrange(desc(number)) %>% filter(terms != "omitted")
```


#create bar chart, reorder and labelling
```{r}
ggplot(freqTermsdf[1:20,], aes(x = reorder(terms,number), y=number)) + geom_bar(stat = "identity", fill = "#9ba7a7") +
  geom_text(aes(label = number),size = 4, hjust = 2, color = "white") + coord_flip() +
  labs(title = "Words with greatest frequency", x = "Number of mentions", y = "terms") +
  geom_hline(yintercept = 0, size =1, colour = "#9ba7a7") +
  coord_flip() + theme_minimal() +
  theme(axis.title.y = element_blank(),
        plot.title = element_text(size = 12, hjust = -.1),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11)) +
  theme(panel.grid.major = element_line(color = "#d8e6e6" ),
        panel.grid.minor = element_line("#d8e6e6")) +
  labs(title = "Words with greatest frequency", x = "Number of mentions", y = "terms")
```
I'm happy to see that "love" shows up at #11! Even after 10 years of marriage, we still say "love you". I'd like to see this go up in the future though. It's interesting that "sorry" comes up at about the same frequency. I think a good dose of apologies or admitting that one messed up is part of a healthy relationship so the two words are not necessarily in conflict. I'm guessing I say "sorry" more just because I'm the one that tends to forget things. I will look into that later.   
Much of our communication is about logistics such as when I'll be leaving work, where I'm going, or when I expect to get home so I'm not surprised to words like "get", "home", "now", or "going" in the top 10. The word "girls" refers to our two daughters. 

##Simple word cloud
```{r}
library(wordcloud)
# create word cloud with three colors (red and gray provide a good contrast for high and low freq words)
wordcloud(freqTermsdf$term, 
          freqTermsdf$num,
          min.freq = 100,
          scale = c(3,.9), # indicates the range of the size of the words
          colors = brewer.pal(8, "Dark2"),
          random.order = FALSE) # random order places terms with highest frequency in the middle

# load the viridisLite package. The viridisLite color schemes are perceptually-uniform, both in regular form and when converted to black-and-white.
# The colors are also designed to be perceived by readers with color blindness.
library(viridisLite)
color_pal <- viridis(n=5)
wordcloud(freqTermsdf$term, 
          freqTermsdf$num,
          min.freq = 100,
          scale = c(3, .9),
          colors = color_pal,
          random.order = FALSE)
```

#compare number of messages by author
```{r, results = 'asis'}
#Use rwa_read function from rwhatsapp package to convert text to dataframe format
texts2 <- rwa_read("whatsapp.txt")

texts2 %>% na.omit() %>% group_by(author) %>% summarize(number = n())


num_texts <- texts2 %>% na.omit() %>% tabyl(author) %>%
  adorn_pct_formatting(digits = 0)

kable(num_texts)

library(ggalt)
num_texts %>%
  ggplot(aes(x = author, y = n)) +
  geom_lollipop(point.size = 5) +
  coord_flip() +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        plot.title = element_text(size = 12, hjust = -.1),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_text(size = 11),
        axis.text.y = element_text(size = 11)) +
  theme(panel.grid.major = element_line(color = "#d8e6e6" ),
        panel.grid.minor = element_line("#d8e6e6")) +
  labs(title = "Comparison of number of texts sent by Hannah versus Charles in the last 2 years")

# percentage more messages sent by Hannah
(8178-5142)/(8178+5142)

```
Clearly, Hannah is  much more prolific on WhatsApp than I am. 61% of all messages sent in the last two years were from Hannah. She texted 23% more than I did.  

#compare texts from Charles and Hannah
```{r}
#separate texts by author
HG <- texts2 %>% filter(author == "Hannah Guedenet")
CG <- texts2 %>% filter(author == "Charles G")
```


#Find common words and create a commonality cloud
```{r}
#Say you want to visualize common words across multiple documents. You can do this with commonality.cloud().
#Each of our HG and CG corpora is composed of many individual messages
#To treat the HG Messages as a single document and likewise for CG, 
#you paste() together all the messages in each corpus along with the parameter collapse = " ".
#This collapses all messages (separated by a space) into a single vector. Then you
#can create a single vector containing all documents.

collapse <- paste(clean_corp, collapse = " ")

#convert to corpus
all_corpus <- VCorpus(VectorSource(collapse))

# Create collapsed vector for HG
all_HG <- paste(HG$text, collapse = " ")

# Create collapsed vector for CG
all_CG <- paste(CG$text, collapse = " ")

# create single collapsed vector with both CG and HG
all_together <- c(all_HG, all_CG)

# Convert to a corpus
all_together <- VCorpus(VectorSource(all_together))

# Clean the corpus
all_clean <- clean_corpus(all_together)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Create all_m
all_m <- as.matrix(all_tdm)
head(all_m)

# Print a commonality cloud
color_pal <- viridis(n=8)
commonality.cloud(all_m, max.words = 20,
                  colors = color_pal,
                  scale = c(3,.5))
```

#using tidytext to compare HG and CG messaging
```{r}
#unnest tokens (separate all messages into words) and summarize by author
compare <- texts2 %>%
  na.omit() %>%
  unnest_tokens(input = "text",
                output = "word") %>%
  count(author, word, sort = TRUE) %>%
  group_by(author) %>%
  filter(!word %in% c(Top200Words,"omit","i'm","it's","don't","media","omitted")) %>%
  top_n(n = 10)

# create facet bar charts
ggplot(compare, aes(x = reorder(word, n), y = n, fill = author)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  ylab("") +
  xlab("") +
  coord_flip() +
  facet_wrap(~author, ncol = 2, scales = "free_y") +
  ggtitle("Most often used words") +
  theme_bw()
```

#create a pyramid chart
```{r}
# The words contained in each document.
# The counts of those words from document 1.
# The counts of those words from document 2
# Keep rows where word appears everywhere

pyr <- texts2 %>%
  na.omit(.) %>%
  unnest_tokens(input = "text", output = "word") %>%
  count(author, word, sort = TRUE) %>% data.frame() %>%
  spread(author,n)

str(pyr)

filter_all(all_vars(. > 0)) %>% 
  # Get difference in counts
  mutate(difference = chardonnay - coffee) %>% 
  # Keep rows with biggest difference
  top_n(25, wt = difference) %>% 
  # Arrange by descending difference
  arrange(desc(difference))

pyramid.plot(
  # Chardonnay counts
  top25_df$chardonnay, 
  # Coffee counts
  top25_df$coffee, 
  # Words
  labels = top25_df$word, 
  top.labels = c("Chardonnay", "Words", "Coffee"), 
  main = "Words in Common", 
  unit = NULL,
  gap = 8,
)
```


# DISSIMILAR COMPARISON CLOUD
##Say you want to visualize the words not in common. To do this, you can also use comparison.cloud() 
#and the steps are quite similar with one main difference.
#Like when you were searching for words in common, you start by unifying the tweets into distinct corpora 
#and combining them into their own VCorpus() object. Next apply a clean_corpus() function and organize it into a TermDocumentMatrix.
#To keep track of what words belong to coffee versus chardonnay, you can set the column names of the TDM 

# polarized tag cloud -----------------------------------------------------

1+1
# POLARIZED TAG CLOUD (pyramid.plot)

# Commonality clouds show words that are shared across documents. One interesting 
#thing that they can't show you is which of those words appear more commonly in 
#one document compared to another. For this, you need a pyramid plot; these can 
#be generated using pyramid.plot() from the plotrix package.

#First, some manipulation is required to get the data in a suitable form. This is most easily 
#done by converting it to a data frame and using dplyr. Given a matrix of word counts, 
#as created by as.matrix(tdm), you need to end up with a data frame with three columns:
  
  # The words contained in each document.
  # The counts of those words from document 1.
  # The counts of those words from document 2

top25_df <- all_tdm_m %>%
  # Convert to data frame
  as_data_frame(rownames = "word") %>% 
  # Keep rows where word appears everywhere
  filter_all(all_vars(. > 0)) %>% 
  # Get difference in counts
  mutate(difference = chardonnay - coffee) %>% 
  # Keep rows with biggest difference
  top_n(25, wt = difference) %>% 
  # Arrange by descending difference
  arrange(desc(difference))

pyramid.plot(
  # Chardonnay counts
  top25_df$chardonnay, 
  # Coffee counts
  top25_df$coffee, 
  # Words
  labels = top25_df$word, 
  top.labels = c("Chardonnay", "Words", "Coffee"), 
  main = "Words in Common", 
  unit = NULL,
  gap = 8,
)

# ----------------------------------------------------------------------------------

# Visualize WOR NETWORKS

#Another way to view word connections is to treat them as a network, similar to a social network. 
#Word networks show term association and cohesion. A word of caution: these visuals can become very dense and hard to interpret visually.

#In a network graph, the circles are called nodes and represent individual terms, 
#while the lines connecting the circles are called edges and represent the connections between the terms.

#For the over-caffeinated text miner, qdap provides a shorcut for making word networks. 
#The word_network_plot() and word_associate() functions both make word networks easy!
  
#The sample code constructs a word network for words associated with "Marvin".

# Word association
word_associate(coffee_tweets$text, match.string = "barista", 
               stopwords = c(Top200Words, "coffee", "amp"), 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Barista Coffee Twee Associations") %>% %>% 
 text <- filter(x=5)

##------------------------------
#use tidytext
library(lubridate) # works well with dates
chat <- rwa_read("whatsapp.txt")  

# peaks and valleys of whatsapp messaging
chat %>%
  mutate(day = date(time)) %>%
  count(day)  %>%
  ggplot(aes(x = day, y = n)) +
  geom_bar(stat = "identity") +
  ylab("") + xlab("")

# Number of messages per author
chat %>% filter(author != "NA") %>%
  mutate(day = date(time)) %>%
  count(author) %>%
  ggplot(aes(x = reorder(author, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Number of messages") +
  geom_text(aes(label = n),size = 4, hjust = 2, color = "white") + coord_flip() +
  geom_hline(yintercept = 0, size =1, colour = "#9ba7a7") +
  theme_minimal() +
  theme(axis.title.y=element_blank(),
        plot.title = element_text(size = 12, hjust = -.3),
        axis.text.x=element_blank(),# axis titles
        axis.title.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y = element_text(size = 12)) +
  theme(panel.grid.major = element_line(color = "#d8e6e6" ),
        panel.grid.minor = element_line("#d8e6e6"))

#Hannah has sent about 62% more text messages than me or over 3,000 more messages! 

https://camo.githubusercontent.com/2b8330206cd79c425a2596e67dbf7032f756d4db/68747470733a2f2f692e696d6775722e636f6d2f6f3345415733782e706e67

https://github.com/sudharsan13296/Whatsapp-analytics/blob/master/WhatsappAnalysis.R

https://rpubs.com/lgeorge2651/151203